{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A automated poet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3Zg9r+/HZ/5ZLJ7KxHQE8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sTHb7nd1b_T",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p369MNUL1gD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yWEhD33V0OF",
        "colab_type": "text"
      },
      "source": [
        "# **Mount Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKK2nQSzV0eU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c96b998f-74fa-4c05-dbde-f4d2811cdc58"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My Drive"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/My Drive'\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sLbzlvE1lDJ",
        "colab_type": "text"
      },
      "source": [
        "# **Loading data**\n",
        "### Here we are using python to read our dataset which is in .txt format. I just give the path of my data which is placed in folder named automated_poet,inside another folder named kaggle in my google drive and the dataset name is sonnet.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smySu5l71oKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "6fb3292c-fd15-4214-863f-6215bc3fb9bd"
      },
      "source": [
        "text=open(\"kaggle/automated_poet/sonnet.txt\").read()\n",
        "text=text.lower()\n",
        "\n",
        "print('Dataset\\n',text[0:1000])\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset\n",
            " i\n",
            "\n",
            " from fairest creatures we desire increase,\n",
            " that thereby beauty's rose might never die,\n",
            " but as the riper should by time decease,\n",
            " his tender heir might bear his memory:\n",
            " but thou, contracted to thine own bright eyes,\n",
            " feed'st thy light's flame with self-substantial fuel,\n",
            " making a famine where abundance lies,\n",
            " thy self thy foe, to thy sweet self too cruel:\n",
            " thou that art now the world's fresh ornament,\n",
            " and only herald to the gaudy spring,\n",
            " within thine own bud buriest thy content,\n",
            " and tender churl mak'st waste in niggarding:\n",
            "   pity the world, or else this glutton be,\n",
            "   to eat the world's due, by the grave and thee.\n",
            "\n",
            " ii\n",
            "\n",
            " when forty winters shall besiege thy brow,\n",
            " and dig deep trenches in thy beauty's field,\n",
            " thy youth's proud livery so gazed on now,\n",
            " will be a tatter'd weed of small worth held:\n",
            " then being asked, where all thy beauty lies,\n",
            " where all the treasure of thy lusty days;\n",
            " to say, within thine own deep sunken eyes,\n",
            " were an all-eating shame, and thriftless praise.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpOTXRCV4cS1",
        "colab_type": "text"
      },
      "source": [
        "# **Creating character/ word mapping**\n",
        "\n",
        "### Here we are making the dataset a set and then making it a list and sorting it alphabetically.\n",
        "First you need to know the difference between set and list.\n",
        "Set is a unordered and unindexed collection and it contains no duplicates.\n",
        "List is ordered , indexed collection and allows duplicate members.\n",
        "\n",
        "So here, first we are removing all the duplicate member by pushing them in a set then we are giving them index by pushing them in a list. and finally we are sorting them alphabetically.\n",
        "\n",
        "**character variable contains a sorted list of letters. Like: [ '  ', ' a ', ' b ', ' c ', ' d ' ].** \n",
        "\n",
        "As we give them index so **n variable contains the index numbers**.  \n",
        "**n_to_char** is dictionary where key in the index numbers and value is the single. characters.\n",
        "**char_to_n** is vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSsRz50H4ftw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "characters = sorted(list(set(text)))\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1sxwyGj1XQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a12dafe1-2276-4fd8-fc15-a12b850beed4"
      },
      "source": [
        "print(characters)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGcKRFxg-ZBj",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preprocessing**\n",
        "\n",
        "### As our model will be generating words. so first we have to train the model which letter can be followed by which one to make a meaningful word. So in this case we are declaring a sequence of letters of length 100. and the next word will be predicted upon this sequence.\n",
        "Let me clear it!\n",
        "\n",
        "\n",
        "\n",
        "1.First 2 empty list X & Y are declared. \n",
        "\n",
        "2.find the length of the dataset.\n",
        "\n",
        "3.There is a for loop which is started from 0 and the condition to iterate through the loop is (length- seq-length) .\n",
        "Say, the length is  1000, seq-length is 100 and as initially i=0.\n",
        "\n",
        "so, **i < (length-sequence)** in 1st iteration and the value of the sequence is getting updated under the loop so next time the value of the sequence will be 200 . and i increases by 1 value.\n",
        "\n",
        "**UNDER THE HOOD ( for loop)**\n",
        "\n",
        "Sequence is a list of alphabets. We are setting the limit by $[i:i+seqlength]$ means [0:100] in first iteration. First 100 letteres will be in the sequence variable. And the 101th one will be in the label .\n",
        "\n",
        "X is containing the features (first 100 words) and Y is containing the label.\n",
        "\n",
        "**EXAMPLE OF X, Y**\n",
        "\n",
        "                                       \n",
        "X=[h, e, l, l]    Y=[o]\n",
        "\n",
        "X=[e, l, l, o]\t  Y=[ ]\n",
        "\n",
        "X=[l, l, o,  ]\t  Y=[i]\n",
        "\n",
        "X=[l, o,  , i]\t  Y=[n] \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LsLRs5E-btn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "length = len(text)\n",
        "seq_length = 100\n",
        "for i in range(0, length-seq_length, 1):\n",
        "     sequence = text[i:i + seq_length]\n",
        "     label =text[i + seq_length]\n",
        "     X.append([char_to_n[char] for char in sequence])\n",
        "     Y.append(char_to_n[label])\n",
        "     "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL9_Ch4Tlqdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c934f361-9cc5-4607-d17f-7e6a8a4f3192"
      },
      "source": [
        "len(X)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97819"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRJmx0VVwrgX",
        "colab_type": "text"
      },
      "source": [
        "# **Reshaping data**\n",
        "X is reshaped in expected dimension. and in the second line we are scaling the X. so that it can be easy for neural network to be trained. And then we are using one hot encoding to encode Y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKfsLQAuE0dJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af526117-4672-48e3-d110-8d28932eec91"
      },
      "source": [
        "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
        "print('X is modified_1',X_modified)\n",
        "X_modified = X_modified / float(len(characters))\n",
        "print('X is modified_2',X_modified)\n",
        "Y_modified = np_utils.to_categorical(Y)\n",
        "print('Y is modified',Y_modified)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X is modified_1 [[[20]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  ...\n",
            "  [12]\n",
            "  [30]\n",
            "  [ 1]]\n",
            "\n",
            " [[ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  ...\n",
            "  [30]\n",
            "  [ 1]\n",
            "  [31]]\n",
            "\n",
            " [[ 0]\n",
            "  [ 1]\n",
            "  [17]\n",
            "  ...\n",
            "  [ 1]\n",
            "  [31]\n",
            "  [19]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[12]\n",
            "  [23]\n",
            "  [23]\n",
            "  ...\n",
            "  [ 1]\n",
            "  [23]\n",
            "  [26]]\n",
            "\n",
            " [[23]\n",
            "  [23]\n",
            "  [ 6]\n",
            "  ...\n",
            "  [23]\n",
            "  [26]\n",
            "  [33]]\n",
            "\n",
            " [[23]\n",
            "  [ 6]\n",
            "  [ 0]\n",
            "  ...\n",
            "  [26]\n",
            "  [33]\n",
            "  [16]]]\n",
            "X is modified_2 [[[0.52631579]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  ...\n",
            "  [0.31578947]\n",
            "  [0.78947368]\n",
            "  [0.02631579]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.02631579]\n",
            "  ...\n",
            "  [0.78947368]\n",
            "  [0.02631579]\n",
            "  [0.81578947]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.02631579]\n",
            "  [0.44736842]\n",
            "  ...\n",
            "  [0.02631579]\n",
            "  [0.81578947]\n",
            "  [0.5       ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.31578947]\n",
            "  [0.60526316]\n",
            "  [0.60526316]\n",
            "  ...\n",
            "  [0.02631579]\n",
            "  [0.60526316]\n",
            "  [0.68421053]]\n",
            "\n",
            " [[0.60526316]\n",
            "  [0.60526316]\n",
            "  [0.15789474]\n",
            "  ...\n",
            "  [0.60526316]\n",
            "  [0.68421053]\n",
            "  [0.86842105]]\n",
            "\n",
            " [[0.60526316]\n",
            "  [0.15789474]\n",
            "  [0.        ]\n",
            "  ...\n",
            "  [0.68421053]\n",
            "  [0.86842105]\n",
            "  [0.42105263]]]\n",
            "Y is modified [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIxtKLtsFVe5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3968b2dd-0f42-454f-fee3-0e91bfafcdbf"
      },
      "source": [
        "X_modified.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97819, 100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5czQgTpNHJ2b",
        "colab_type": "text"
      },
      "source": [
        "# **Model**\n",
        "Our Lstm model has 700 unit in input layer and input shape (100, 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6myi3NKoHLmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(700, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(700))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyrOJS0qPzO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8bd773be-daaf-457e-9700-b412f907e553"
      },
      "source": [
        "model.fit(X_modified, Y_modified, epochs=1, batch_size=100)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "97819/97819 [==============================] - 7367s 75ms/step - loss: 2.8641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fe54a6d46d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sI-TrlMsBB",
        "colab_type": "text"
      },
      "source": [
        "# **generating text**\n",
        "\n",
        "Here we are Predicting next 100 words. First 10 words are given. This process is similar like the data preprocessing part and reshaping part of the training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_97EL1hMt-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7473eee4-aa14-434e-b336-5b1fb29d99d4"
      },
      "source": [
        "\n",
        "string_mapped = X[10]\n",
        "full_string = [n_to_char[value] for value in string_mapped]\n",
        "print(full_string)\n",
        "# generating characters\n",
        "for i in range(seq_length):\n",
        "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
        "    x = x / float(len(characters))\n",
        "    #print(x.shape)\n",
        "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
        "    seq = [n_to_char[value] for value in string_mapped]\n",
        "    full_string.append(n_to_char[pred_index])\n",
        "    string_mapped.append(pred_index)\n",
        "    string_mapped = string_mapped[1:len(string_mapped)]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'i', 'r', 'e', 's', 't', ' ', 'c', 'r', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', 'w', 'e', ' ', 'd', 'e', 's', 'i', 'r', 'e', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', ',', '\\n', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', 'r', 'e', 'b', 'y', ' ', 'b', 'e', 'a', 'u', 't', 'y', \"'\", 's', ' ', 'r', 'o', 's', 'e', ' ', 'm', 'i', 'g', 'h', 't', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 'd', 'i', 'e', ',', '\\n', ' ', 'b', 'u', 't', ' ', 'a', 's', ' ', 't', 'h', 'e', ' ', 'r', 'i', 'p', 'e', 'r', ' ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El54XwgGNiwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dc22aa93-ac8d-4c62-eb0c-6ac7cd6a1b2b"
      },
      "source": [
        "txt=''\n",
        "for char in full_string:\n",
        "   txt = txt+char\n",
        "txt"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"airest creatures we desire increase,\\n that thereby beauty's rose might never die,\\n but as the riper the the the thee                   th the the the the the the the the the the the the the the the th\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}